{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a234f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this notebook I train a neural network to predict experimental bandgap. \n",
    "# I have modified the approach taught in Dr Alex Ganose's Data Analytics module so that the nn parameters are assigned in a class\n",
    "# I then train the model using cross-validation to assess it's performance\n",
    "# MAE (0.47) is worse than XGBoost benchmark (0.43), but that it expected for supervised learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "897d4808",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error \n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03dbe552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load experimental data into pandas dataframe\n",
    "\n",
    "df = pd.read_csv(\"team-a.csv\")\n",
    "df = df.drop(['formula'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f40c44b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split experimental data\n",
    "\n",
    "X = df[['MagpieData maximum MendeleevNumber', 'MagpieData mean AtomicWeight',\n",
    "       'MagpieData minimum MeltingT', 'MagpieData maximum MeltingT',\n",
    "       'MagpieData mean MeltingT', 'MagpieData minimum Column',\n",
    "       'MagpieData range Column', 'MagpieData avg_dev Column',\n",
    "       'MagpieData mode Column', 'MagpieData range Row', 'MagpieData mean Row',\n",
    "       'MagpieData range Electronegativity',\n",
    "       'MagpieData avg_dev Electronegativity',\n",
    "       'MagpieData mode Electronegativity', 'MagpieData mean NpValence',\n",
    "       'MagpieData maximum NdValence', 'MagpieData range NdValence',\n",
    "       'MagpieData mean NdValence', 'MagpieData maximum NfValence',\n",
    "       'MagpieData mean NfValence', 'MagpieData mean NValence',\n",
    "       'MagpieData mode NValence', 'MagpieData maximum NpUnfilled',\n",
    "       'MagpieData range NpUnfilled', 'MagpieData mean NpUnfilled',\n",
    "       'MagpieData range NUnfilled', 'MagpieData mean NUnfilled',\n",
    "       'MagpieData mode NUnfilled', 'MagpieData minimum GSvolume_pa',\n",
    "       'MagpieData mode GSvolume_pa', 'MagpieData maximum GSbandgap',\n",
    "       'MagpieData range GSbandgap', 'MagpieData mode GSbandgap',\n",
    "       'MagpieData mean GSmagmom', 'MagpieData mode SpaceGroupNumber']].values\n",
    "\n",
    "\n",
    "y = df['gap expt'].values\n",
    "y = y.reshape(-1,1)\n",
    "\n",
    "# some dodgy variable names here - but simpler for implementing cross val later, just want to save x_val and y_val for later\n",
    "\n",
    "X,X_val_holdout,y,y_val_holdout = train_test_split(X,y,test_size=0.2,random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0e85f4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try and define nn as a class to make it easier to work with\n",
    "# i think hidden_size = 128 and num_classes = 1\n",
    "\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.fc4 = nn.Linear(32, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc4(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f16b5a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up k-fold cross validation\n",
    "\n",
    "k = 5\n",
    "kf = KFold(n_splits=k,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "03924401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold validation MAE:0.4925520122051239\n",
      "fold validation MAE:0.47664642333984375\n",
      "fold validation MAE:0.4583447575569153\n",
      "fold validation MAE:0.45019763708114624\n",
      "fold validation MAE:0.4806918799877167\n",
      "MAE:0.47168654203414917\n",
      "mae standard deviation:0.015367762185633183\n"
     ]
    }
   ],
   "source": [
    "# initialise and train model\n",
    "# key point here - need to split data, scale, covert to tensors, and load for each fold for cross validation, then train model. \n",
    "\n",
    "input_size = X.shape[1]\n",
    "hidden_size = 128\n",
    "num_classes = 1\n",
    "\n",
    "val_losses = []\n",
    "\n",
    "for train_index, val_index in kf.split(X):\n",
    "    X_train, X_val = X[train_index], X[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "    # scale data. be careful, need seperate x scaler and y scaler because they are different shapes\n",
    "    # also watch out for fit_transform for the train data and .transform for test. fit_transform calculates mean/std from data as well as tranformation \n",
    "    scaler_X = StandardScaler()\n",
    "    scaler_y = StandardScaler()\n",
    "\n",
    "    X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "    y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1,1))\n",
    "    X_val_holdout_scaled = scaler_X.transform(X_val_holdout)\n",
    "    y_val_holdout_scaled = scaler_y.transform(y_val_holdout.reshape(-1,1))\n",
    "\n",
    "    # convert to tensors\n",
    "    X_train_tensor = torch.FloatTensor(X_train_scaled)\n",
    "    y_train_tensor = torch.FloatTensor(y_train_scaled)\n",
    "    X_val_tensor = torch.FloatTensor(X_val_holdout_scaled)\n",
    "    y_val_tensor = torch.FloatTensor(y_val_holdout_scaled)\n",
    "\n",
    "    # create dataloader - this feeds data in batches during training \n",
    "    train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor),batch_size=32,shuffle=True)\n",
    "\n",
    "    # create model\n",
    "    model = SimpleNet(input_size, hidden_size, num_classes)\n",
    "\n",
    "    # use different criterion for classification problems. this optimiser is standard, can change lr if results are poor\n",
    "    criterion = nn.L1Loss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    # training loop\n",
    "    num_epochs = 100\n",
    "    for epoch in range(num_epochs):\n",
    "        # proper implementation of dataloader:\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # evaluation - calculates accuracies and adds to list initialised at start\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_val_tensor)\n",
    "        outputs_original = scaler_y.inverse_transform(outputs.numpy())\n",
    "        y_val_original = scaler_y.inverse_transform(y_val_tensor.numpy())\n",
    "\n",
    "        val_loss = np.mean(np.abs(outputs_original -y_val_original))\n",
    "        val_losses.append(val_loss)\n",
    "        print(f\"fold validation MAE:{val_loss.item()}\")\n",
    "            \n",
    "# calculates accuracy\n",
    "avg_val_loss = np.mean(val_losses)\n",
    "print(f\"MAE:{avg_val_loss}\")\n",
    "print(f\"mae standard deviation:{np.std(val_losses)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
