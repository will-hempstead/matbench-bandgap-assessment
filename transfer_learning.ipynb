{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53cdfe46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error \n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9570154e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load experimental data into pandas dataframe\n",
    "\n",
    "df_exp = pd.read_csv(\"team-a.csv\")\n",
    "df_exp = df_exp.drop(['formula'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "33af6cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split experimental data\n",
    "\n",
    "X_exp = df_exp[['MagpieData maximum MendeleevNumber', 'MagpieData mean AtomicWeight',\n",
    "       'MagpieData minimum MeltingT', 'MagpieData maximum MeltingT',\n",
    "       'MagpieData mean MeltingT', 'MagpieData minimum Column',\n",
    "       'MagpieData range Column', 'MagpieData avg_dev Column',\n",
    "       'MagpieData mode Column', 'MagpieData range Row', 'MagpieData mean Row',\n",
    "       'MagpieData range Electronegativity',\n",
    "       'MagpieData avg_dev Electronegativity',\n",
    "       'MagpieData mode Electronegativity', 'MagpieData mean NpValence',\n",
    "       'MagpieData maximum NdValence', 'MagpieData range NdValence',\n",
    "       'MagpieData mean NdValence', 'MagpieData maximum NfValence',\n",
    "       'MagpieData mean NfValence', 'MagpieData mean NValence',\n",
    "       'MagpieData mode NValence', 'MagpieData maximum NpUnfilled',\n",
    "       'MagpieData range NpUnfilled', 'MagpieData mean NpUnfilled',\n",
    "       'MagpieData range NUnfilled', 'MagpieData mean NUnfilled',\n",
    "       'MagpieData mode NUnfilled', 'MagpieData minimum GSvolume_pa',\n",
    "       'MagpieData mode GSvolume_pa', 'MagpieData maximum GSbandgap',\n",
    "       'MagpieData range GSbandgap', 'MagpieData mode GSbandgap',\n",
    "       'MagpieData mean GSmagmom', 'MagpieData mode SpaceGroupNumber']].values\n",
    "\n",
    "\n",
    "y_exp = df_exp['gap expt'].values\n",
    "y_exp = y_exp.reshape(-1,1)\n",
    "X_train_exp,X_val_exp_holdout,y_train_exp,y_val_exp_holdout = train_test_split(X_exp,y_exp,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092eec42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dft bandgap dataset\n",
    "\n",
    "df_dft = pd.read_csv(\"dft_bandgap_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e066a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract important features based on importance in xgboost model and load data\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_dft = df_dft[['MagpieData maximum MendeleevNumber', 'MagpieData mean AtomicWeight',\n",
    "       'MagpieData minimum MeltingT', 'MagpieData maximum MeltingT',\n",
    "       'MagpieData mean MeltingT', 'MagpieData minimum Column',\n",
    "       'MagpieData range Column', 'MagpieData avg_dev Column',\n",
    "       'MagpieData mode Column', 'MagpieData range Row', 'MagpieData mean Row',\n",
    "       'MagpieData range Electronegativity',\n",
    "       'MagpieData avg_dev Electronegativity',\n",
    "       'MagpieData mode Electronegativity', 'MagpieData mean NpValence',\n",
    "       'MagpieData maximum NdValence', 'MagpieData range NdValence',\n",
    "       'MagpieData mean NdValence', 'MagpieData maximum NfValence',\n",
    "       'MagpieData mean NfValence', 'MagpieData mean NValence',\n",
    "       'MagpieData mode NValence', 'MagpieData maximum NpUnfilled',\n",
    "       'MagpieData range NpUnfilled', 'MagpieData mean NpUnfilled',\n",
    "       'MagpieData range NUnfilled', 'MagpieData mean NUnfilled',\n",
    "       'MagpieData mode NUnfilled', 'MagpieData minimum GSvolume_pa',\n",
    "       'MagpieData mode GSvolume_pa', 'MagpieData maximum GSbandgap',\n",
    "       'MagpieData range GSbandgap', 'MagpieData mode GSbandgap',\n",
    "       'MagpieData mean GSmagmom', 'MagpieData mode SpaceGroupNumber']].values\n",
    "y_dft = df_dft['gap pbe'].values\n",
    "y_dft = y_dft.reshape(-1,1)\n",
    "\n",
    "X_train_dft,X_val_dft_holdout,y_train_dft,y_val_dft_holdout = train_test_split(X_dft,y_dft,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ae75fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.fc4 = nn.Linear(32, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc4(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a8c06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up k-fold cross validation\n",
    "\n",
    "k = 3\n",
    "kf = KFold(n_splits=k,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "95181bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold validation MAE:0.4490479826927185\n",
      "fold validation MAE:0.4519434869289398\n",
      "fold validation MAE:0.447721391916275\n",
      "fold validation MAE:0.4412199854850769\n",
      "fold validation MAE:0.4541642665863037\n",
      "MAE:0.4488193988800049\n",
      "mae standard deviation:0.004412176087498665\n"
     ]
    }
   ],
   "source": [
    "# initialise and train model\n",
    "# key point here - need to split data, scale, covert to tensors, and load for each fold for cross validation, then train model. \n",
    "\n",
    "input_size = X_train_dft.shape[1]\n",
    "hidden_size = 128\n",
    "num_classes = 1\n",
    "\n",
    "val_losses = []\n",
    "\n",
    "for train_index, val_index in kf.split(X_train_dft):\n",
    "    X_train, X_val = X_train_dft[train_index], X_train_dft[val_index]\n",
    "    y_train, y_val = y_train_dft[train_index], y_train_dft[val_index]\n",
    "\n",
    "    # scale data. be careful, need seperate x scaler and y scaler because they are different shapes\n",
    "    # also watch out for fit_transform for the train data and .transform for test. fit_transform calculates mean/std from data as well as tranformation \n",
    "    scaler_X = StandardScaler()\n",
    "    scaler_y = StandardScaler()\n",
    "\n",
    "    X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "    y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1,1))\n",
    "    X_val_holdout_scaled = scaler_X.transform(X_val_dft_holdout)\n",
    "    y_val_holdout_scaled = scaler_y.transform(y_val_dft_holdout.reshape(-1,1))\n",
    "\n",
    "    # convert to tensors\n",
    "    X_train_tensor = torch.FloatTensor(X_train_scaled)\n",
    "    y_train_tensor = torch.FloatTensor(y_train_scaled)\n",
    "    X_val_tensor = torch.FloatTensor(X_val_holdout_scaled)\n",
    "    y_val_tensor = torch.FloatTensor(y_val_holdout_scaled)\n",
    "\n",
    "    # create dataloader - this feeds data in batches during training \n",
    "    train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor),batch_size=32,shuffle=True)\n",
    "\n",
    "    # create model\n",
    "    model = SimpleNet(input_size, hidden_size, num_classes)\n",
    "\n",
    "    # use different criterion for classification problems. this optimiser is standard, can change lr if results are poor\n",
    "    criterion = nn.L1Loss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # training loop\n",
    "    num_epochs = 50\n",
    "    for epoch in range(num_epochs):\n",
    "        # proper implementation of dataloader:\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # evaluation - calculates accuracies and adds to list initialised at start\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_val_tensor)\n",
    "        outputs_original = scaler_y.inverse_transform(outputs.numpy())\n",
    "        y_val_original = scaler_y.inverse_transform(y_val_tensor.numpy())\n",
    "\n",
    "        val_loss = np.mean(np.abs(outputs_original -y_val_original))\n",
    "        val_losses.append(val_loss)\n",
    "        print(f\"fold validation MAE:{val_loss.item()}\")\n",
    "            \n",
    "# calculates accuracy\n",
    "avg_val_loss = np.mean(val_losses)\n",
    "print(f\"MAE:{avg_val_loss}\")\n",
    "print(f\"mae standard deviation:{np.std(val_losses)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f44e478a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold validation MAE:0.4685835838317871\n",
      "fold validation MAE:0.42799195647239685\n",
      "fold validation MAE:0.44363340735435486\n",
      "fold validation MAE:0.44315817952156067\n",
      "fold validation MAE:0.4302848279476166\n",
      "MAE:0.4427303671836853\n",
      "mae standard deviation:0.014432597905397415\n"
     ]
    }
   ],
   "source": [
    "# now to fine-tune model trained on dft bandgap to work on experimental bandgap:\n",
    "\n",
    "# freeze layers of pretrained NN:\n",
    "\n",
    "# Freeze all except the last layer\n",
    "for i, p in enumerate(model.parameters()):\n",
    "    p.requires_grad = True  # start by enabling all\n",
    "# If your last layer is model[-1], freeze earlier layers:\n",
    "for p in list(model.parameters())[:-2]:  # heuristic: freeze everything except last Linear weights+bias\n",
    "    p.requires_grad = False\n",
    "\n",
    "# train on dataset:\n",
    "\n",
    "input_size = X_train_exp.shape[1]\n",
    "hidden_size = 128\n",
    "num_classes = 1\n",
    "\n",
    "val_losses = []\n",
    "\n",
    "for train_index, val_index in kf.split(X_train_exp):\n",
    "    X_train, X_val = X_train_exp[train_index], X_train_exp[val_index]\n",
    "    y_train, y_val = y_train_exp[train_index], y_train_exp[val_index]\n",
    "\n",
    "    # scale data. be careful, need seperate x scaler and y scaler because they are different shapes\n",
    "    # also watch out for fit_transform for the train data and .transform for test. fit_transform calculates mean/std from data as well as tranformation \n",
    "    scaler_X = StandardScaler()\n",
    "    scaler_y = StandardScaler()\n",
    "\n",
    "    X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "    y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1,1))\n",
    "    X_val_holdout_scaled = scaler_X.transform(X_val_exp_holdout)\n",
    "    y_val_holdout_scaled = scaler_y.transform(y_val_exp_holdout.reshape(-1,1))\n",
    "\n",
    "    # convert to tensors\n",
    "    X_train_tensor = torch.FloatTensor(X_train_scaled)\n",
    "    y_train_tensor = torch.FloatTensor(y_train_scaled)\n",
    "    X_val_tensor = torch.FloatTensor(X_val_holdout_scaled)\n",
    "    y_val_tensor = torch.FloatTensor(y_val_holdout_scaled)\n",
    "\n",
    "    # create dataloader - this feeds data in batches during training \n",
    "    train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor),batch_size=32,shuffle=True)\n",
    "\n",
    "    # create model\n",
    "    model = SimpleNet(input_size, hidden_size, num_classes)\n",
    "\n",
    "    # crucial - use different optimiser with much lower learning rate\n",
    "    criterion = nn.L1Loss()\n",
    "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "    # training loop\n",
    "    num_epochs = 100\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # evaluation - calculates accuracies and adds to list initialised at start\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_val_tensor)\n",
    "        outputs_original = scaler_y.inverse_transform(outputs.numpy())\n",
    "        y_val_original = scaler_y.inverse_transform(y_val_tensor.numpy())\n",
    "\n",
    "        val_loss = np.mean(np.abs(outputs_original -y_val_original))\n",
    "        val_losses.append(val_loss)\n",
    "        print(f\"fold validation MAE:{val_loss.item()}\")\n",
    "            \n",
    "# calculates accuracy\n",
    "avg_val_loss = np.mean(val_losses)\n",
    "print(f\"MAE:{avg_val_loss}\")\n",
    "print(f\"mae standard deviation:{np.std(val_losses)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777bf485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# good MAE for a first attempt and proves concept that transfer learning could work for these datasets. \n",
    "# ideas for next steps:\n",
    "# - explore the dft bandgap dataset a bit more, does it contain a load of organics that are not relevent for our inorganic dataset for example?\n",
    "# - try fine-tuning the fine-tuning - changing no. of epochs to '100' and increasing learning rate has just returned a better model than the NN trained only on bandgap data!! great result\n",
    "# - have a look at if there is something wrong with the model, since it is better at using dft bandgap with fine-tuning on experimental bandgap, than it is at predicting dft bandgap "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
