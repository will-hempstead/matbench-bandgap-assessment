{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53cdfe46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error \n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9570154e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load experimental data into pandas dataframe\n",
    "\n",
    "df_exp = pd.read_csv(\"team-a.csv\")\n",
    "df_exp = df_exp.drop(['formula'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "33af6cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split experimental data\n",
    "\n",
    "X_exp = df_exp[['MagpieData maximum MendeleevNumber', 'MagpieData mean AtomicWeight',\n",
    "       'MagpieData minimum MeltingT', 'MagpieData maximum MeltingT',\n",
    "       'MagpieData mean MeltingT', 'MagpieData minimum Column',\n",
    "       'MagpieData range Column', 'MagpieData avg_dev Column',\n",
    "       'MagpieData mode Column', 'MagpieData range Row', 'MagpieData mean Row',\n",
    "       'MagpieData range Electronegativity',\n",
    "       'MagpieData avg_dev Electronegativity',\n",
    "       'MagpieData mode Electronegativity', 'MagpieData mean NpValence',\n",
    "       'MagpieData maximum NdValence', 'MagpieData range NdValence',\n",
    "       'MagpieData mean NdValence', 'MagpieData maximum NfValence',\n",
    "       'MagpieData mean NfValence', 'MagpieData mean NValence',\n",
    "       'MagpieData mode NValence', 'MagpieData maximum NpUnfilled',\n",
    "       'MagpieData range NpUnfilled', 'MagpieData mean NpUnfilled',\n",
    "       'MagpieData range NUnfilled', 'MagpieData mean NUnfilled',\n",
    "       'MagpieData mode NUnfilled', 'MagpieData minimum GSvolume_pa',\n",
    "       'MagpieData mode GSvolume_pa', 'MagpieData maximum GSbandgap',\n",
    "       'MagpieData range GSbandgap', 'MagpieData mode GSbandgap',\n",
    "       'MagpieData mean GSmagmom', 'MagpieData mode SpaceGroupNumber']].values\n",
    "\n",
    "\n",
    "y_exp = df_exp['gap expt'].values\n",
    "y_exp = y_exp.reshape(-1,1)\n",
    "X_train_exp,X_test_exp,y_train_exp,y_test_exp = train_test_split(X_exp,y_exp,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "142dddc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: matbench_mp_gap\n",
      "Description: Matbench v0.1 test dataset for predicting DFT PBE band gap from structure. Adapted from Materials Project database. Removed entries having a formation energy (or energy above the convex hull) more than 150meV and those containing noble gases. Retrieved April 2, 2019. For benchmarking w/ nested cross validation, the order of the dataset must be identical to the retrieved data; refer to the Automatminer/Matbench publication for more details.\n",
      "Columns:\n",
      "\tgap pbe: Target variable. The band gap as calculated by PBE DFT from the Materials Project, in eV.\n",
      "\tstructure: Pymatgen Structure of the material.\n",
      "Num Entries: 106113\n",
      "Reference: A. Jain*, S.P. Ong*, G. Hautier, W. Chen, W.D. Richards, S. Dacek, S. Cholia, D. Gunter, D. Skinner, G. Ceder, K.A. Persson (*=equal contributions)\n",
      "The Materials Project: A materials genome approach to accelerating materials innovation\n",
      "APL Materials, 2013, 1(1), 011002.\n",
      "doi:10.1063/1.4812323\n",
      "Bibtex citations: [\"@Article{Dunn2020,\\nauthor={Dunn, Alexander\\nand Wang, Qi\\nand Ganose, Alex\\nand Dopp, Daniel\\nand Jain, Anubhav},\\ntitle={Benchmarking materials property prediction methods: the Matbench test set and Automatminer reference algorithm},\\njournal={npj Computational Materials},\\nyear={2020},\\nmonth={Sep},\\nday={15},\\nvolume={6},\\nnumber={1},\\npages={138},\\nabstract={We present a benchmark test suite and an automated machine learning procedure for evaluating supervised machine learning (ML) models for predicting properties of inorganic bulk materials. The test suite, Matbench, is a set of 13{\\\\thinspace}ML tasks that range in size from 312 to 132k samples and contain data from 10 density functional theory-derived and experimental sources. Tasks include predicting optical, thermal, electronic, thermodynamic, tensile, and elastic properties given a material's composition and/or crystal structure. The reference algorithm, Automatminer, is a highly-extensible, fully automated ML pipeline for predicting materials properties from materials primitives (such as composition and crystal structure) without user intervention or hyperparameter tuning. We test Automatminer on the Matbench test suite and compare its predictive power with state-of-the-art crystal graph neural networks and a traditional descriptor-based Random Forest model. We find Automatminer achieves the best performance on 8 of 13 tasks in the benchmark. We also show our test suite is capable of exposing predictive advantages of each algorithm---namely, that crystal graph methods appear to outperform traditional machine learning methods given {\\\\textasciitilde}104 or greater data points. We encourage evaluating materials ML algorithms on the Matbench benchmark and comparing them against the latest version of Automatminer.},\\nissn={2057-3960},\\ndoi={10.1038/s41524-020-00406-3},\\nurl={https://doi.org/10.1038/s41524-020-00406-3}\\n}\\n\", '@article{Jain2013,\\nauthor = {Jain, Anubhav and Ong, Shyue Ping and Hautier, Geoffroy and Chen, Wei and Richards, William Davidson and Dacek, Stephen and Cholia, Shreyas and Gunter, Dan and Skinner, David and Ceder, Gerbrand and Persson, Kristin a.},\\ndoi = {10.1063/1.4812323},\\nissn = {2166532X},\\njournal = {APL Materials},\\nnumber = {1},\\npages = {011002},\\ntitle = {{The Materials Project: A materials genome approach to accelerating materials innovation}},\\nurl = {http://link.aip.org/link/AMPADS/v1/i1/p011002/s1\\\\&Agg=doi},\\nvolume = {1},\\nyear = {2013}\\n}']\n",
      "File type: json.gz\n",
      "Figshare URL: https://ml.materialsproject.org/projects/matbench_mp_gap.json.gz\n",
      "SHA256 Hash Digest: 58b65746bd88329986ed66031a2ac1369c7c522f7bc9f9081528e07097c2c057\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>structure</th>\n",
       "      <th>gap pbe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[-0.00812638  0.02476014 -0.01698117] K, [-0....</td>\n",
       "      <td>1.3322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[0.         1.78463544 1.78463544] Cr, [1.784...</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[-2.13764909 -2.12540569 -2.14704542] Cs, [-6...</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[0. 0. 0.] Si, [ 4.55195829  4.55195829 -4.55...</td>\n",
       "      <td>0.4113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[0.    2.655 2.655] Ca, [2.655 0.    2.655] C...</td>\n",
       "      <td>0.3514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106108</th>\n",
       "      <td>[[ 2.91058377  3.61215869 -0.19100541] Ca, [-0...</td>\n",
       "      <td>1.1354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106109</th>\n",
       "      <td>[[0.07215014 3.75835129 1.91249744] Ta, [2.014...</td>\n",
       "      <td>2.7274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106110</th>\n",
       "      <td>[[0.99954964 0.70129827 4.70919163] Mg, [ 0.87...</td>\n",
       "      <td>2.8860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106111</th>\n",
       "      <td>[[0.99298226 0.71146045 4.70710628] Zn, [ 0.86...</td>\n",
       "      <td>2.2330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106112</th>\n",
       "      <td>[[ 7.28898036  5.15386774 12.6253607 ] Mg, [1....</td>\n",
       "      <td>1.0583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>106113 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                structure  gap pbe\n",
       "0       [[-0.00812638  0.02476014 -0.01698117] K, [-0....   1.3322\n",
       "1       [[0.         1.78463544 1.78463544] Cr, [1.784...   0.0000\n",
       "2       [[-2.13764909 -2.12540569 -2.14704542] Cs, [-6...   0.0000\n",
       "3       [[0. 0. 0.] Si, [ 4.55195829  4.55195829 -4.55...   0.4113\n",
       "4       [[0.    2.655 2.655] Ca, [2.655 0.    2.655] C...   0.3514\n",
       "...                                                   ...      ...\n",
       "106108  [[ 2.91058377  3.61215869 -0.19100541] Ca, [-0...   1.1354\n",
       "106109  [[0.07215014 3.75835129 1.91249744] Ta, [2.014...   2.7274\n",
       "106110  [[0.99954964 0.70129827 4.70919163] Mg, [ 0.87...   2.8860\n",
       "106111  [[0.99298226 0.71146045 4.70710628] Zn, [ 0.86...   2.2330\n",
       "106112  [[ 7.28898036  5.15386774 12.6253607 ] Mg, [1....   1.0583\n",
       "\n",
       "[106113 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dft data into pandas dataframe\n",
    "\n",
    "from matminer.datasets import load_dataset, get_all_dataset_info\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")  # ignore warnings during featurisation\n",
    "\n",
    "print(get_all_dataset_info(\"matbench_mp_gap\"))\n",
    "\n",
    "df_dft = load_dataset(\"matbench_mp_gap\")\n",
    "df_dft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef410237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3c0946b62304dcb9ae5f71483e9fa18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ElementProperty:   0%|          | 0/106113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# generate magpie features for data \n",
    "\n",
    "from pymatgen.core import Composition\n",
    "from matminer.featurizers.composition import ElementProperty\n",
    "\n",
    "df_dft[\"composition\"] = df_dft[\"structure\"].apply(lambda s: s.composition)\n",
    "\n",
    "ep = ElementProperty.from_preset(\"magpie\")\n",
    "ep.set_n_jobs(1)\n",
    "ep.featurize_dataframe(df_dft, col_id=\"composition\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e066a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract important features based on importance in xgboost model and load data\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_dft = df_dft[['MagpieData maximum MendeleevNumber', 'MagpieData mean AtomicWeight',\n",
    "       'MagpieData minimum MeltingT', 'MagpieData maximum MeltingT',\n",
    "       'MagpieData mean MeltingT', 'MagpieData minimum Column',\n",
    "       'MagpieData range Column', 'MagpieData avg_dev Column',\n",
    "       'MagpieData mode Column', 'MagpieData range Row', 'MagpieData mean Row',\n",
    "       'MagpieData range Electronegativity',\n",
    "       'MagpieData avg_dev Electronegativity',\n",
    "       'MagpieData mode Electronegativity', 'MagpieData mean NpValence',\n",
    "       'MagpieData maximum NdValence', 'MagpieData range NdValence',\n",
    "       'MagpieData mean NdValence', 'MagpieData maximum NfValence',\n",
    "       'MagpieData mean NfValence', 'MagpieData mean NValence',\n",
    "       'MagpieData mode NValence', 'MagpieData maximum NpUnfilled',\n",
    "       'MagpieData range NpUnfilled', 'MagpieData mean NpUnfilled',\n",
    "       'MagpieData range NUnfilled', 'MagpieData mean NUnfilled',\n",
    "       'MagpieData mode NUnfilled', 'MagpieData minimum GSvolume_pa',\n",
    "       'MagpieData mode GSvolume_pa', 'MagpieData maximum GSbandgap',\n",
    "       'MagpieData range GSbandgap', 'MagpieData mode GSbandgap',\n",
    "       'MagpieData mean GSmagmom', 'MagpieData mode SpaceGroupNumber']].values\n",
    "y_dft = df_dft['gap pbe'].values\n",
    "y_dft = y_dft.reshape(-1,1)\n",
    "\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "X_scaled = scaler_X.fit_transform(X_dft)\n",
    "y_scaled = scaler_y.fit_transform(y_dft)\n",
    "\n",
    "X_train_dft,X_val_dft,y_train_dft,y_val_dft = train_test_split(X_scaled,y_scaled,test_size=0.2,random_state=42)\n",
    "\n",
    "X_train_tensor = torch.FloatTensor(X_train_dft)\n",
    "y_train_tensor = torch.FloatTensor(y_train_dft)\n",
    "\n",
    "X_val_tensor = torch.FloatTensor(X_val_dft)\n",
    "y_val_tensor = torch.FloatTensor(y_val_dft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95181bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataloader\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=34, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb2427c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some model parameters\n",
    "\n",
    "input_size = X_train_dft.shape[1]\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(input_size, 128),   \n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 1)  \n",
    ")\n",
    "\n",
    "# and optimiser\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b5c68ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Train loss: 0.4336, Val loss: 0.3771\n",
      "Epoch [2/100], Train loss: 0.3647, Val loss: 0.3602\n",
      "Epoch [3/100], Train loss: 0.3452, Val loss: 0.3408\n",
      "Epoch [4/100], Train loss: 0.3304, Val loss: 0.3364\n",
      "Epoch [5/100], Train loss: 0.3193, Val loss: 0.3365\n",
      "Epoch [6/100], Train loss: 0.3110, Val loss: 0.3171\n",
      "Epoch [7/100], Train loss: 0.3018, Val loss: 0.3199\n",
      "Epoch [8/100], Train loss: 0.2955, Val loss: 0.2984\n",
      "Epoch [9/100], Train loss: 0.2899, Val loss: 0.3031\n",
      "Epoch [10/100], Train loss: 0.2841, Val loss: 0.2918\n",
      "Epoch [11/100], Train loss: 0.2787, Val loss: 0.3038\n",
      "Epoch [12/100], Train loss: 0.2757, Val loss: 0.3048\n",
      "Epoch [13/100], Train loss: 0.2692, Val loss: 0.2838\n",
      "Epoch [14/100], Train loss: 0.2648, Val loss: 0.2776\n",
      "Epoch [15/100], Train loss: 0.2634, Val loss: 0.2783\n",
      "Epoch [16/100], Train loss: 0.2593, Val loss: 0.2798\n",
      "Epoch [17/100], Train loss: 0.2555, Val loss: 0.2727\n",
      "Epoch [18/100], Train loss: 0.2534, Val loss: 0.2675\n",
      "Epoch [19/100], Train loss: 0.2505, Val loss: 0.2743\n",
      "Epoch [20/100], Train loss: 0.2483, Val loss: 0.2685\n",
      "Epoch [21/100], Train loss: 0.2441, Val loss: 0.2642\n",
      "Epoch [22/100], Train loss: 0.2427, Val loss: 0.2580\n",
      "Epoch [23/100], Train loss: 0.2394, Val loss: 0.2681\n",
      "Epoch [24/100], Train loss: 0.2373, Val loss: 0.2626\n",
      "Epoch [25/100], Train loss: 0.2364, Val loss: 0.2643\n",
      "Epoch [26/100], Train loss: 0.2327, Val loss: 0.2596\n",
      "Epoch [27/100], Train loss: 0.2320, Val loss: 0.2610\n",
      "Epoch [28/100], Train loss: 0.2296, Val loss: 0.2586\n",
      "Epoch [29/100], Train loss: 0.2268, Val loss: 0.2542\n",
      "Epoch [30/100], Train loss: 0.2249, Val loss: 0.2479\n",
      "Epoch [31/100], Train loss: 0.2228, Val loss: 0.2515\n",
      "Epoch [32/100], Train loss: 0.2214, Val loss: 0.2515\n",
      "Epoch [33/100], Train loss: 0.2182, Val loss: 0.2486\n",
      "Epoch [34/100], Train loss: 0.2180, Val loss: 0.2418\n",
      "Epoch [35/100], Train loss: 0.2160, Val loss: 0.2479\n",
      "Epoch [36/100], Train loss: 0.2139, Val loss: 0.2508\n",
      "Epoch [37/100], Train loss: 0.2134, Val loss: 0.2406\n",
      "Epoch [38/100], Train loss: 0.2118, Val loss: 0.2568\n",
      "Epoch [39/100], Train loss: 0.2107, Val loss: 0.2462\n",
      "Epoch [40/100], Train loss: 0.2087, Val loss: 0.2409\n",
      "Epoch [41/100], Train loss: 0.2065, Val loss: 0.2396\n",
      "Epoch [42/100], Train loss: 0.2053, Val loss: 0.2367\n",
      "Epoch [43/100], Train loss: 0.2035, Val loss: 0.2431\n",
      "Epoch [44/100], Train loss: 0.2040, Val loss: 0.2409\n",
      "Epoch [45/100], Train loss: 0.2027, Val loss: 0.2356\n",
      "Epoch [46/100], Train loss: 0.2003, Val loss: 0.2399\n",
      "Epoch [47/100], Train loss: 0.1998, Val loss: 0.2348\n",
      "Epoch [48/100], Train loss: 0.1984, Val loss: 0.2437\n",
      "Epoch [49/100], Train loss: 0.1989, Val loss: 0.2450\n",
      "Epoch [50/100], Train loss: 0.1972, Val loss: 0.2360\n",
      "Epoch [51/100], Train loss: 0.1960, Val loss: 0.2382\n",
      "Epoch [52/100], Train loss: 0.1941, Val loss: 0.2311\n",
      "Epoch [53/100], Train loss: 0.1924, Val loss: 0.2360\n",
      "Epoch [54/100], Train loss: 0.1939, Val loss: 0.2394\n",
      "Epoch [55/100], Train loss: 0.1914, Val loss: 0.2416\n",
      "Epoch [56/100], Train loss: 0.1915, Val loss: 0.2372\n",
      "Epoch [57/100], Train loss: 0.1884, Val loss: 0.2329\n",
      "Epoch [58/100], Train loss: 0.1888, Val loss: 0.2304\n",
      "Epoch [59/100], Train loss: 0.1877, Val loss: 0.2273\n",
      "Epoch [60/100], Train loss: 0.1873, Val loss: 0.2275\n",
      "Epoch [61/100], Train loss: 0.1856, Val loss: 0.2373\n",
      "Epoch [62/100], Train loss: 0.1851, Val loss: 0.2279\n",
      "Epoch [63/100], Train loss: 0.1847, Val loss: 0.2279\n",
      "Epoch [64/100], Train loss: 0.1825, Val loss: 0.2361\n",
      "Epoch [65/100], Train loss: 0.1838, Val loss: 0.2362\n",
      "Epoch [66/100], Train loss: 0.1819, Val loss: 0.2326\n",
      "Epoch [67/100], Train loss: 0.1815, Val loss: 0.2299\n",
      "Epoch [68/100], Train loss: 0.1814, Val loss: 0.2270\n",
      "Epoch [69/100], Train loss: 0.1797, Val loss: 0.2260\n",
      "Epoch [70/100], Train loss: 0.1787, Val loss: 0.2248\n",
      "Epoch [71/100], Train loss: 0.1782, Val loss: 0.2299\n",
      "Epoch [72/100], Train loss: 0.1765, Val loss: 0.2228\n",
      "Epoch [73/100], Train loss: 0.1762, Val loss: 0.2314\n",
      "Epoch [74/100], Train loss: 0.1761, Val loss: 0.2263\n",
      "Epoch [75/100], Train loss: 0.1760, Val loss: 0.2300\n",
      "Epoch [76/100], Train loss: 0.1737, Val loss: 0.2227\n",
      "Epoch [77/100], Train loss: 0.1745, Val loss: 0.2278\n",
      "Epoch [78/100], Train loss: 0.1737, Val loss: 0.2227\n",
      "Epoch [79/100], Train loss: 0.1727, Val loss: 0.2202\n",
      "Epoch [80/100], Train loss: 0.1716, Val loss: 0.2228\n",
      "Epoch [81/100], Train loss: 0.1701, Val loss: 0.2245\n",
      "Epoch [82/100], Train loss: 0.1704, Val loss: 0.2228\n",
      "Epoch [83/100], Train loss: 0.1702, Val loss: 0.2229\n",
      "Epoch [84/100], Train loss: 0.1701, Val loss: 0.2263\n",
      "Epoch [85/100], Train loss: 0.1693, Val loss: 0.2187\n",
      "Epoch [86/100], Train loss: 0.1674, Val loss: 0.2293\n",
      "Epoch [87/100], Train loss: 0.1686, Val loss: 0.2225\n",
      "Epoch [88/100], Train loss: 0.1681, Val loss: 0.2195\n",
      "Epoch [89/100], Train loss: 0.1665, Val loss: 0.2231\n",
      "Epoch [90/100], Train loss: 0.1672, Val loss: 0.2274\n",
      "Epoch [91/100], Train loss: 0.1658, Val loss: 0.2236\n",
      "Epoch [92/100], Train loss: 0.1661, Val loss: 0.2238\n",
      "Epoch [93/100], Train loss: 0.1651, Val loss: 0.2190\n",
      "Epoch [94/100], Train loss: 0.1646, Val loss: 0.2220\n",
      "Epoch [95/100], Train loss: 0.1637, Val loss: 0.2229\n",
      "Epoch [96/100], Train loss: 0.1642, Val loss: 0.2189\n",
      "Epoch [97/100], Train loss: 0.1632, Val loss: 0.2211\n",
      "Epoch [98/100], Train loss: 0.1621, Val loss: 0.2193\n",
      "Epoch [99/100], Train loss: 0.1614, Val loss: 0.2168\n",
      "Epoch [100/100], Train loss: 0.1617, Val loss: 0.2183\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "# Number of complete passes through the dataset\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "# keep track of the loss for each epoch\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Start the training loop\n",
    "\n",
    "for epoch in range(num_epochs): \n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(X_batch)\n",
    "        loss = criterion(predictions, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    # Iterate over the validation data and compute the loss\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "\n",
    "    # turn off gradients since we are in the evaluation mode\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_val, y_val in val_loader:\n",
    "            val_predictions = model(X_val)\n",
    "            loss = criterion(val_predictions, y_val)\n",
    "\n",
    "            # Add the loss for this batch to the validation loss\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    train_losses.append(train_loss/len(train_loader))\n",
    "    val_losses.append(val_loss/len(val_loader))\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train loss: {train_losses[-1]:.4f}, Val loss: {val_losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0290af68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation MAE: 0.4535\n"
     ]
    }
   ],
   "source": [
    "# calculate mae, dont fully understand this code -check\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Get all predictions\n",
    "all_predictions = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in val_loader: \n",
    "        predictions = model(X_batch)\n",
    "        all_predictions.append(predictions)\n",
    "        all_targets.append(y_batch)\n",
    "\n",
    "# Concatenate all batches\n",
    "all_predictions = torch.cat(all_predictions)\n",
    "all_targets = torch.cat(all_targets)\n",
    "\n",
    "# Convert to numpy and unscale (IMPORTANT!)\n",
    "predictions_scaled = all_predictions.numpy()\n",
    "targets_scaled = all_targets.numpy()\n",
    "\n",
    "# Inverse transform to get original scale\n",
    "predictions_original = scaler_y.inverse_transform(predictions_scaled)\n",
    "targets_original = scaler_y.inverse_transform(targets_scaled)\n",
    "\n",
    "# Calculate MAE\n",
    "mae = mean_absolute_error(targets_original, predictions_original)\n",
    "print(f\"Validation MAE: {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6c040af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = scaler_X.transform(X_train_exp)\n",
    "X_test_scaled  = scaler_X.transform(X_test_exp)\n",
    "\n",
    "# Scale y using ORIGINAL target scaler so the output space matches the pretrained head\n",
    "y_train_scaled = scaler_y.transform(y_train_exp)\n",
    "y_test_scaled  = scaler_y.transform(y_test_exp)\n",
    "\n",
    "# Tensors with consistent shapes\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_scaled, dtype=torch.float32)  # shape (n,1)\n",
    "X_test_tensor  = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_test_tensor  = torch.tensor(y_test_scaled, dtype=torch.float32)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader_expt = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ff347b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/50], Loss (scaled): 0.153631\n",
      "Epoch [10/50], Loss (scaled): 0.139267\n",
      "Epoch [20/50], Loss (scaled): 0.134614\n",
      "Epoch [30/50], Loss (scaled): 0.132693\n",
      "Epoch [40/50], Loss (scaled): 0.131463\n"
     ]
    }
   ],
   "source": [
    "# train pretrained model on new data\n",
    "\n",
    "# Freeze all except the last layer\n",
    "for i, p in enumerate(model.parameters()):\n",
    "    p.requires_grad = True  # start by enabling all\n",
    "# If your last layer is model[-1], freeze earlier layers:\n",
    "for p in list(model.parameters())[:-2]:  # heuristic: freeze everything except last Linear weights+bias\n",
    "    p.requires_grad = False\n",
    "\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4, weight_decay=1e-4)\n",
    "criterion = nn.SmoothL1Loss(beta=1.0)  # Huber loss for robustness\n",
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for X_batch, y_batch in train_loader_expt:\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(X_batch)\n",
    "        if preds.ndim == 1:\n",
    "            preds = preds.view(-1, 1)\n",
    "        loss = criterion(preds, y_batch)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        avg_loss = train_loss / len(train_loader_expt)\n",
    "        print(f\"Epoch [{epoch}/{num_epochs}], Loss (scaled): {avg_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "260cc978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE (scaled space): 0.339961\n",
      "Fine-tuned MAE: 0.5436\n",
      "XGBoost baseline: 0.42\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    preds_test = model(X_test_tensor)\n",
    "    if preds_test.ndim == 1:\n",
    "        preds_test = preds_test.view(-1, 1)\n",
    "\n",
    "# Debug: MAE in scaled space\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "mae_scaled = mean_absolute_error(y_test_tensor.numpy(), preds_test.numpy())\n",
    "print(f\"MAE (scaled space): {mae_scaled:.6f}\")\n",
    "\n",
    "# Inverse-transform with ORIGINAL target scaler to get eV\n",
    "preds_original   = scaler_y.inverse_transform(preds_test.numpy())\n",
    "targets_original = y_test_exp  # already in eV, no inverse needed if we didn't re-scale y with a new scaler\n",
    "\n",
    "# If you followed section C, y_test_exp was scaled with scaler_y; in that case:\n",
    "# targets_original = scaler_y.inverse_transform(y_test_tensor.numpy())\n",
    "\n",
    "mae_finetuned = mean_absolute_error(targets_original, preds_original)\n",
    "print(f\"Fine-tuned MAE: {mae_finetuned:.4f}\")\n",
    "print(f\"XGBoost baseline: 0.42\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ca5095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now approaching mae for xgboost baseline using transfer learning!!! next steps:\n",
    "# - try different fine-tuning parameters, can i integrate it to try a load of different parameters and return the best?\n",
    "# - try improving the pre-trained NN, again can i build a better optimiser?\n",
    "# - is there a better dataset available than dft bandgap? check email thing "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
